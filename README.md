# Movies-ETL
![mod8.png](PNGs/mod8.png)


## Overview

The purpose of this analysis was to learn the process of creating data pipelines, also known as Extract, Transform, Load (ETL). This process, which is often the first step before performing any analysis, consists of moving robust data around between databases––it ensures that the data is consistent and maintains its integrity. 

For this assignment, ETL was performed on several movie datasets to predict popular films for a streaming service. An algorithm was created to predict which low budget movies being released would become popular in order to purchase at a low price. 

## Analysis

ETL pipelines were created from raw data to a SQL database. The data was extracted from different sources using Python, and cleaned and transformed using Pandas. The last step was to ensure the data was transformed into a consistent structure and then loaded to a data target. PostgreSQL, a relational database, was used for this purpose.

The iterative process for cleaning the data was broken down as follows:

- Inspecting the data
- Identifying problems with the data
- Making a plan and deciding if it was worth the time to fix it
- Executing the data



## Resources

- Python and Pandas
- PostgreSQL and pgAdmin
- Three CSV files  
1. Movies_metadata.csv
2. Wikipedia-movies.JSON
3. Ratings.csv





